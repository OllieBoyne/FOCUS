<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html">

<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-L9CZWSSL8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-L9CZWSSL8V');
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>FOCUS</title>
    <link rel="shortcut icon" type="image/ico" href="images/logos/icon.ico" />
    <link rel="icon" type="image/ico" href="images/logos/icon.ico" />
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script src="https://kit.fontawesome.com/49f46e7382.js" crossorigin="anonymous"></script>
</head>

<body>
    <nav class="navbar is-light" role="navigation" aria-label="main navigation">
        <div class="container is-max-desktop">
            <div class="navbar-brand">
                <a class="navbar-item" href="http://www.eng.cam.ac.uk/">
                    <img src="images/logos/Cam_bw.png" alt="University of Cambridge" style="height: 2.0rem;">
                </a>
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div id="navbarBasicExample" class="navbar-menu">
                <div class="navbar-end">
                    <a class="navbar-item navbar-right" href="https://3dvconf.github.io/2025/">
                        <img src="images/logos/3dv2025_banner_cropped.png" alt="3DV 2025" style="height: 2.0rem;">
                    </a>
                </div>
            </div>
        </div>
    </nav>
    <section class="section">
        <div class="container is-max-desktop">
            <div class="container is-max-desktop">
                <figure class="image">
                    <img src="images/logos/focus_v1.png" class="scaled_center_img" style="max-height: 100px;width: auto;height: auto"/>
                </figure>

            </div>

            <h1 class="title is-2 is-size-3-mobile is-spaced has-text-centered">
                FOCUS - Multi-View Foot Reconstruction from Synthetically Trained Dense Correspondences

            </h1>
            <p class="subtitle is-5 has-text-centered has-text-grey">
                <a href="https://3dvconf.github.io/2025/" style="color:grey;">3DV 2025</a>
            </p>
            <p class="subtitle is-6 has-text-centered authors mt-5" style="line-height: 1.5;">
                <span>
                    <a href="https://ollieboyne.github.io">Oliver Boyne</a>
                </span>
                <span>
                    <a href="https://mi.eng.cam.ac.uk/~cipolla/">Roberto&nbsp;Cipolla</a>
                </span>
            </p>
            <p class="subtitle is-6 has-text-centered authors mt-5" style="line-height: 1.5;">
                <span>University of Cambridge</span>
            </p>
        </div>


        <!-- LINKS -->
        <div class="container is-max-desktop has-text-centered mt-5">
            <table class="center">
                <tr>
                    <td><a href="https://arxiv.org/abs/2502.06367" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
            </a></td>
                    <td> <a href="https://github.com/OllieBoyne/FOCUS" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="fab fa-github"></i></span><span>Code</span>
            </a></td>

                    <td> <a href="https://github.com/OllieBoyne/SynFoot" class="button is-rounded is-link is-light mr-2">
                        <span class="icon"><img src="images/logos/synfoot_icon_v1.png"/></span><span>Synthetic dataset</span></a> </td>

                    <td> <a href="https://github.com/OllieBoyne/Foot3D" class="button is-rounded is-link is-light mr-2">
                        <span class="icon"><img src="images/logos/synfoot_icon_v1.png"/></span><span>Reconstruction dataset</span></a> </td>

                </tr>
            </table>
        </div>
        </section>

<!--    HEADLINE-->
        <section class="section pt-0">
            <div class="container is-max-desktop">
                <h1>At a glance</h1>
                <div class="content has-text-justified-desktop">

        <video id="videoPlayer" autoplay loop muted playsinline>
          <source src="videos/splash_small.webm" type="video/webm">
            <source src="videos/splash.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>

                    <ul>
                        <li>We introduce 2 methods to <b>reconstruct feet from multiview images</b>. These methods can run on <b>uncalibrated image sets</b>, <b>videos</b>, and can run in <b>under a minute</b>, with <b>no GPU</b>*.</li>
                        <li> We extend synthetic dataset <a href="http://github.com/OllieBoyne/SynFoot" class="text-link">SynFoot</a> to include dense correspondences.</li>
                        <li> We train a predictor to predict these correspondences on in-the-wild images.</li>
                        <li> We recover a 3D model from multi-view images using the correspondences by either (i) matching and triangulation or (ii) fitting the <a href="http://ollieboyne.github.io/FIND" class="text-link">FIND</a> model.</li>
                    </ul>


                </div>
                <p>*FOCUS-SfM only.</p>
            </div>
        </section>

        <!--    Synthetics-->
        <section class="section pt-0">
            <div class="container is-max-desktop">
                <h1>Synthetic data</h1>

                <div class="content has-text-justified-desktop">

                 <p>We extend the synthetic dataset SynFoot to SynFoot2, including more diversity, articulated feet, and dense correspondences..</p>

                <p>These were created using our custom library <a href="http://ollieboyne.github.io/BlenderSynth" class="text-link">BlenderSynth</a>,
                and are <a href="http://github.com/OllieBoyne/SynFoot" class="text-link">available for download</a>.</p>

<!--                <figure>-->
<!--                  <a href="http://github.com/OllieBoyne/SynFoot"><img src="images/results/synth_samples.png" alt="Samples from our synthetic dataset"/></a>-->
<!--                  <figcaption><strong>Samples from our synthetic dataset. Left-to-right: RGB, mask, surface normals, keypoints.</strong></figcaption>-->
<!--                </figure>-->

                <figure>
                    <div class="overlay-manager">
                        <div class="radio-buttons" id="radio-buttons-synth">
                              <!-- JavaScript will populate this -->
                            </div>
                        <input type="range" id="slider-synth" min="0" max="100" step="1" value="0">

                    </div>
                    <div class="image-wrapper" id="image-wrapper-synth">
                          <!-- JavaScript will populate this -->
                    </div>
                    <figcaption><strong>Samples from synthetic dataset (drag the slider to view)</strong></figcaption>
                </figure>

                </div>



            </div>
        </section>

        <!--    Surface normals-->
        <section class="section pt-0">
            <div class="container is-max-desktop">
                <h1>Dense correspondence prediction</h1>

                <div class="content has-text-justified-desktop">

                <p>We train a network to predict dense correspondences, surface normals, and related uncertainties.</p>
                <p>See our predictions on in-the-wild images below.</p>

                <figure>
                    <div class="overlay-manager">
                        <div class="radio-buttons" id="radio-buttons-normals">
                              <!-- JavaScript will populate this -->
                            </div>
                        <input type="range" id="slider-normals" min="0" max="100" step="1" value="0">

                    </div>
                    <div class="image-wrapper" id="image-wrapper-normals">
                          <!-- JavaScript will populate this -->
                    </div>
                    <figcaption><strong>In-the-wild predictions (drag the slider to view)</strong></figcaption>
                </figure>

                </div>

            </div>
        </section>


        <!--    Fitting-->
        <section class="section pt-0">
            <div class="container is-max-desktop">
                <h1>3D Reconstruction</h1>

                <div class="content has-text-justified-desktop">

                    <p>We introduce 2 methods to fit to our dense correspondences:</p>

                    <p><b>FOCUS-SfM</b>: Collect correspondences across views, triangulate, and run <a href="https://www.cs.jhu.edu/~misha/MyPapers/ToG13.pdf" class="text-link">Poisson reconstruction</a> to recover a surface.</p>
                    <p><b>FOCUS-O</b>: Fit the parameterized <a href="https://www.ollieboyne.com/FIND" class="text-link">FIND</a> model directly to the dense correspondences.</p>

                <figure>
                  <img src="images/other/method_overviews.png"/>
<!--                  <figcaption><strong>...</figcaption>-->
                </figure>


                <p>Our methods produce better surface normal reconstructions than <a href="https://colmap.github.io" class="text-link">COLMAP</a>, evaluated on the <a href="https://github.com/OllieBoyne/Foot3D", class="text-link">Foot3D</a> benchmark foot reconstruction dataset.</p>
                <p>Our methods require as few as 3 views, whereas <a href="https://colmap.github.io" class="text-link">COLMAP</a> needs 15+.</p>


                <figure>
                  <img src="images/results/reconstr_qualitative.png"/>
                  <figcaption><strong>Our methods provide a better reconstruction than FOUND and COLMAP.</strong></figcaption>
                </figure>


                <figure >
                  <img src="images/tables/3d_reconstruction_results.png" style="width:75%"/>
                    <figcaption><strong>Our methods beat COLMAP on surface normal reconstruction, and FOUND on chamfer error.</strong></figcaption>
                </figure>

                <figure>
                    <img src="images/tables/inference_info.png" style="width:75%"/>
                    <figcaption><strong>Our methods run faster, and do not require differentiable rendering. FOCUS-SfM can even run without a GPU.</strong></figcaption>
                </figure>


                <p>Thanks to our dense correspondences, we can reconstruct from a completely uncalibrated imageset - even when backgrounds are untextured and typical camera calibration would fail. Try our demo on <a href="https://github.com/OllieBoyne/FOCUS" class="text-link">Github</a>.</p>

                <img src="images/results/focus_sfm_ollie_carpet_2.gif" style="width: 100%;"/>

                </div>

            </div>
        </section>



    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Acknowledgements
            </h1>

            <div class="content has-text-justified-desktop">
            <p>We acknowledge the collaboration and financial support of <a href="https://snapfeet.io/en/" class="text-link">Trya Srl</a>.</p>
            </div>

            <div class="content has-text-justified-desktop">
                If you make use of this project, please cite the following paper:
            </div>

            <pre>@inproceedings{boyne2025focus,
            title={FOCUS: Multi-View Foot Reconstruction from Synthetically Trained Dense Correspondences},
            author={Boyne, Oliver and Cipolla, Roberto},
            booktitle={2025 International Conference on 3D Vision (3DV)},
            year={2025}
}</pre>

        </div>
    </section>


    <footer class="footer">
        <div class="content has-text-centered">
            <p>
                <img src="images/logos/Cam_bw.png" class="mt-5" alt="University of Cambridge" style="height: 2rem;">
            </p>
        </div>
    </footer>

<script src="slider.js"></script>
</body>

<script>
    document.addEventListener('DOMContentLoaded', () => {

        // Get all "navbar-burger" elements
        const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);

        // Check if there are any navbar burgers
        if ($navbarBurgers.length > 0) {

            // Add a click event on each of them
            $navbarBurgers.forEach(el => {
                el.addEventListener('click', () => {

                    // Get the target from the "data-target" attribute
                    const target = el.dataset.target;
                    const $target = document.getElementById(target);

                    // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
                    el.classList.toggle('is-active');
                    $target.classList.toggle('is-active');

                });
            });
        }
    });
</script>

</html>